{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4bcabb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§­ Using directories:\n",
      "  PROJECT_ROOT : /Users/girithchoudhary/Documents/morro/f1/f1-prediction\n",
      "  DATA_DIR     : /Users/girithchoudhary/Documents/morro/f1/f1-prediction/data\n",
      "  ARTIFACTS_DIR: /Users/girithchoudhary/Documents/morro/f1/f1-prediction/artifacts\n",
      "  HELPER_DIR   : /Users/girithchoudhary/Documents/morro/f1/f1-prediction/server/helper\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Cell 1 â€” Robust paths for local project layout\n",
    "# Works whether you run the notebook from repo root or /server\n",
    "# =====================================================\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "# Candidate roots to search (current dir, parent, grandparent)\n",
    "_CWD = Path.cwd().resolve()\n",
    "_candidates = [_CWD, _CWD.parent, _CWD.parent.parent]\n",
    "\n",
    "def _find_project_root():\n",
    "    for base in _candidates:\n",
    "        if (base / \"data\" / \"constructors.csv\").exists():\n",
    "            return base\n",
    "    # last resort: look for a 'data' dir containing a few expected files\n",
    "    for base in _candidates:\n",
    "        d = base / \"data\"\n",
    "        if d.exists() and (d / \"results.csv\").exists() and (d / \"races.csv\").exists():\n",
    "            return base\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate project root with a ./data folder containing the F1 CSVs.\\n\"\n",
    "        f\"Checked: {[str(p) for p in _candidates]}\"\n",
    "    )\n",
    "\n",
    "PROJECT_ROOT = _find_project_root()\n",
    "\n",
    "# Define canonical paths relative to project root\n",
    "DATA_DIR       = PROJECT_ROOT / \"data\"               # <-- your CSVs live here\n",
    "ARTIFACTS_DIR  = PROJECT_ROOT / \"artifacts\"\n",
    "HELPER_DIR     = PROJECT_ROOT / \"server\" / \"helper\"\n",
    "MODEL_PATH     = ARTIFACTS_DIR / \"finish_regressor_xgb.pkl\"\n",
    "SCHEMA_PATH    = ARTIFACTS_DIR / \"schema_contract.json\"\n",
    "\n",
    "# Create output dirs if needed\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HELPER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ðŸ§­ Using directories:\")\n",
    "print(\"  PROJECT_ROOT :\", PROJECT_ROOT)\n",
    "print(\"  DATA_DIR     :\", DATA_DIR)\n",
    "print(\"  ARTIFACTS_DIR:\", ARTIFACTS_DIR)\n",
    "print(\"  HELPER_DIR   :\", HELPER_DIR)\n",
    "\n",
    "# Quick guard: ensure a few core CSVs exist\n",
    "_required = [\"constructors.csv\", \"drivers.csv\", \"races.csv\", \"circuits.csv\", \"results.csv\", \"pit_stops.csv\"]\n",
    "missing = [f for f in _required if not (DATA_DIR / f).exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing CSVs in {DATA_DIR}: {missing}\\n\"\n",
    "                            \"Make sure your Kaggle dataset is extracted into the project's ./data folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "79822955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 2. Load core CSVs from ./data\n",
    "# =====================================================\n",
    "constructors = pd.read_csv(DATA_DIR / \"constructors.csv\")\n",
    "drivers      = pd.read_csv(DATA_DIR / \"drivers.csv\")\n",
    "races        = pd.read_csv(DATA_DIR / \"races.csv\")\n",
    "circuits     = pd.read_csv(DATA_DIR / \"circuits.csv\")\n",
    "results      = pd.read_csv(DATA_DIR / \"results.csv\")\n",
    "pit_stops    = pd.read_csv(DATA_DIR / \"pit_stops.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f30f2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- circuit_laps.json (robust; derives laps from results.csv) ---\n",
    "# For each race, compute the laps as the maximum laps completed by any classified finisher.\n",
    "race_laps = (\n",
    "    results[results['positionOrder'] > 0]           # only classified finishes\n",
    "        .groupby('raceId', as_index=False)['laps']\n",
    "        .max()\n",
    "        .rename(columns={'laps': 'race_laps'})\n",
    ")\n",
    "\n",
    "# Join to races to get circuitId, then to circuits for name/country\n",
    "circuit_meta = (\n",
    "    races[['raceId', 'circuitId']]\n",
    "        .merge(race_laps, on='raceId', how='left')\n",
    "        .merge(circuits[['circuitId', 'name', 'country']], on='circuitId', how='left')\n",
    ")\n",
    "\n",
    "# Median race_laps per circuit â†’ avgLaps\n",
    "circuit_meta = (\n",
    "    circuit_meta\n",
    "        .groupby(['circuitId', 'name', 'country'], as_index=False)['race_laps']\n",
    "        .median()\n",
    "        .rename(columns={'race_laps': 'avgLaps', 'name': 'name_circuit'})\n",
    ")\n",
    "\n",
    "# Fill any missing with overall median as a fallback\n",
    "circuit_meta['avgLaps'] = circuit_meta['avgLaps'].fillna(circuit_meta['avgLaps'].median())\n",
    "\n",
    "# Save for API\n",
    "with open(HELPER_DIR / \"circuit_laps.json\", \"w\") as f:\n",
    "    json.dump(circuit_meta.to_dict(orient='records'), f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "dd2de49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- overtake_index.json ---\n",
    "# use a dedicated variable name to avoid re-using 'res'\n",
    "res_movement = results.merge(races[['raceId','circuitId','year']], on='raceId', how='left')\n",
    "res_movement = res_movement[(res_movement['grid'] > 0) & (res_movement['positionOrder'] > 0)]\n",
    "res_movement['pos_gain'] = res_movement['grid'] - res_movement['positionOrder']\n",
    "\n",
    "race_movement = (res_movement.groupby(['raceId','circuitId'], as_index=False)['pos_gain']\n",
    "                   .apply(lambda s: float(np.mean(np.abs(s)))))\n",
    "race_movement.rename(columns={'pos_gain':'abs_movement'}, inplace=True)\n",
    "\n",
    "circ_movement = race_movement.groupby('circuitId', as_index=False)['abs_movement'].mean()\n",
    "vmin, vmax = circ_movement['abs_movement'].min(), circ_movement['abs_movement'].max()\n",
    "circ_movement['overtakeIndex'] = (circ_movement['abs_movement'] - vmin) / (vmax - vmin + 1e-9)\n",
    "overtake_index = circ_movement[['circuitId','overtakeIndex']]\n",
    "\n",
    "# save helper file\n",
    "with open(HELPER_DIR / \"overtake_index.json\", \"w\") as f:\n",
    "    json.dump(overtake_index.to_dict(orient='records'), f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "dd61bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Car Performance Index â€” Qualifying *time*-based (constructor/year)\n",
    "# =====================================================\n",
    "qualifying = pd.read_csv(DATA_DIR / \"qualifying.csv\")\n",
    "\n",
    "def _to_ms(x):\n",
    "    if pd.isna(x): \n",
    "        return np.nan\n",
    "    s = str(x).strip()\n",
    "    try:\n",
    "        if \":\" in s:\n",
    "            m, rest = s.split(\":\")\n",
    "            return (int(m) * 60.0 + float(rest)) * 1000.0\n",
    "        return float(s) * 1000.0\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Convert q1/q2/q3 to milliseconds and take the best per driver\n",
    "for col in [\"q1\", \"q2\", \"q3\"]:\n",
    "    qualifying[col + \"_ms\"] = qualifying[col].map(_to_ms)\n",
    "qualifying[\"bestQ_ms\"] = qualifying[[\"q1_ms\", \"q2_ms\", \"q3_ms\"]].min(axis=1)\n",
    "\n",
    "# Attach constructorId to each qualifying row (qualifying doesnâ€™t have it)\n",
    "drv_cons = results[[\"raceId\", \"driverId\", \"constructorId\"]].drop_duplicates()\n",
    "q = (qualifying\n",
    "     .merge(drv_cons, on=[\"raceId\", \"driverId\"], how=\"left\")\n",
    "     .merge(races[[\"raceId\", \"year\"]], on=\"raceId\", how=\"left\"))\n",
    "\n",
    "# Ensure a single constructorId column after merges\n",
    "if \"constructorId\" in q.columns:\n",
    "    pass  # already good (comes from qualifying.csv)\n",
    "else:\n",
    "    # If duplicates exist, prefer qualifying's (usually _x), then fill from results' (_y)\n",
    "    cand_x = \"constructorId_x\" if \"constructorId_x\" in q.columns else None\n",
    "    cand_y = \"constructorId_y\" if \"constructorId_y\" in q.columns else None\n",
    "\n",
    "    if cand_x or cand_y:\n",
    "        q[\"constructorId\"] = np.nan\n",
    "        if cand_x:\n",
    "            q[\"constructorId\"] = q[cand_x]\n",
    "        if cand_y:\n",
    "            q[\"constructorId\"] = q[\"constructorId\"].fillna(q[cand_y])\n",
    "\n",
    "        # clean up extras\n",
    "        drop_cols = [c for c in [cand_x, cand_y] if c]\n",
    "        q.drop(columns=drop_cols, inplace=True)\n",
    "    else:\n",
    "        raise RuntimeError(\"constructorId not found after merges; check input files/merges.\")\n",
    "\n",
    "\n",
    "# For each race & team, keep the team's best qualifying time (fastest driver of that team)\n",
    "# IMPORTANT: include 'year' in the group keys so we don't lose it\n",
    "team_best = (q.dropna(subset=[\"bestQ_ms\"])\n",
    "               .groupby([\"raceId\", \"year\", \"constructorId\"], as_index=False)[\"bestQ_ms\"]\n",
    "               .min())\n",
    "\n",
    "# For each season, derive a constructor pace index from median of race-best times\n",
    "cons_season = (team_best\n",
    "               .groupby([\"year\", \"constructorId\"], as_index=False)[\"bestQ_ms\"]\n",
    "               .median()\n",
    "               .rename(columns={\"bestQ_ms\": \"med_bestQ_ms\"}))\n",
    "\n",
    "# Season-wise min-max to [0,1], where 1.0 = fastest in that season\n",
    "season_minmax = cons_season.groupby(\"year\")[\"med_bestQ_ms\"].agg([\"min\", \"max\"]).reset_index()\n",
    "cons_season = cons_season.merge(season_minmax, on=\"year\", how=\"left\")\n",
    "rng = (cons_season[\"max\"] - cons_season[\"min\"]).replace(0, 1.0)\n",
    "cons_season[\"carPerformanceIndex\"] = 1.0 - ((cons_season[\"med_bestQ_ms\"] - cons_season[\"min\"]) / rng)\n",
    "\n",
    "# Definitive CPI table for downstream merges\n",
    "season_cons_pts = cons_season[[\"year\", \"constructorId\", \"carPerformanceIndex\"]].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1aa7c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 5. Pit features (count, durations, stints)\n",
    "# =====================================================\n",
    "ps = pit_stops.copy()\n",
    "ps['milliseconds'] = ps['milliseconds'].fillna(0).astype(float)\n",
    "agg = (ps.groupby(['raceId','driverId'], as_index=False)\n",
    "         .agg(pit_count=('stop','count'),\n",
    "              pit_total_duration=('milliseconds','sum'),\n",
    "              pit_avg_duration=('milliseconds','mean'),\n",
    "              first_pit_lap=('lap','min'),\n",
    "              last_pit_lap=('lap','max')))\n",
    "\n",
    "def proxy_tire_score(nstops):\n",
    "    if pd.isna(nstops) or nstops == 0: return 1.5\n",
    "    if nstops == 1: return 2.0\n",
    "    if nstops == 2: return 2.4\n",
    "    return 2.7\n",
    "agg['avgTireScore'] = agg['pit_count'].apply(proxy_tire_score)\n",
    "\n",
    "# --- ðŸ‘‡ ADD CHANGE 4 here ---\n",
    "# Tire strategy aggressiveness proxy\n",
    "agg['stints'] = (agg['pit_count'].fillna(0) + 1).clip(1, 5)\n",
    "agg['tire_aggr_index'] = agg['stints'] / agg['pit_total_duration'].replace(0, np.nan)\n",
    "agg['tire_aggr_index'] = agg['tire_aggr_index'].fillna(agg['tire_aggr_index'].median())\n",
    "# --- ðŸ‘† END CHANGE 4 ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f999ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 6. Build training dataset\n",
    "# =====================================================\n",
    "Y = results.merge(races[['raceId','year','round','circuitId']], on='raceId', how='left')\n",
    "Y = Y.merge(circuits[['circuitId','country']], on='circuitId', how='left')\n",
    "Y = Y.merge(overtake_index, on='circuitId', how='left')\n",
    "Y = Y.merge(agg, on=['raceId','driverId'], how='left')\n",
    "Y = Y.merge(season_cons_pts, on=['year','constructorId'], how='left')\n",
    "\n",
    "# Fill missing\n",
    "for c in ['pit_count','pit_total_duration','pit_avg_duration','first_pit_lap','last_pit_lap','avgTireScore']:\n",
    "    Y[c] = Y[c].fillna(0 if c!='avgTireScore' else 1.8)\n",
    "Y['circuit_overtake_difficulty'] = Y['overtakeIndex'].fillna(Y['overtakeIndex'].median())\n",
    "Y['carPerformanceIndex'] = Y['carPerformanceIndex'].fillna(Y['carPerformanceIndex'].median())\n",
    "\n",
    "# --- ðŸ‘‡ ADD CHANGE 3 here ---\n",
    "# first_stop_delta = normalized pit timing (how early the first stop is vs race length)\n",
    "Y = Y.merge(circuit_meta[['circuitId','avgLaps']], on='circuitId', how='left')\n",
    "Y['first_stop_delta'] = np.where(\n",
    "    (Y['avgLaps'].notna()) & (Y['first_pit_lap'] > 0),\n",
    "    Y['first_pit_lap'] / Y['avgLaps'],\n",
    "    0.0\n",
    ")\n",
    "# --- ðŸ‘† END CHANGE 3 ---\n",
    "\n",
    "rounds_per_year = Y.groupby('year', as_index=False)['round'].max().rename(columns={'round':'round_max'})\n",
    "Y = Y.merge(rounds_per_year, on='year', how='left')\n",
    "Y['season_progress'] = (Y['round'] - 1) / (Y['round_max'] - 1 + 1e-9)\n",
    "\n",
    "TARGET = 'positionOrder'\n",
    "FEATURES = [\n",
    " 'grid','pit_count','pit_total_duration','pit_avg_duration',\n",
    " 'first_pit_lap','last_pit_lap','circuit_overtake_difficulty',\n",
    " 'round','circuitId','country','carPerformanceIndex','avgTireScore',\n",
    " 'season_progress','first_stop_delta', 'tire_aggr_index'   # ðŸ‘ˆ add new feature here\n",
    "]\n",
    "dataset = Y[Y[TARGET] > 0][FEATURES + [TARGET]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3f9c444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 6b. Tire strategy features (per driver/race)\n",
    "# =====================================================\n",
    "pit_stops  = pd.read_csv(DATA_DIR / \"pit_stops.csv\")   # raceId, driverId, lap, duration, etc.\n",
    "lap_times  = pd.read_csv(DATA_DIR / \"lap_times.csv\")   # raceId, driverId, lap, position, time\n",
    "# Some Kaggle dumps include 'compound' in other tables; if you donâ€™t have it, skip compound shares.\n",
    "\n",
    "# Stint count = (#pit_stops + 1)\n",
    "stints = (pit_stops.groupby([\"raceId\",\"driverId\"], as_index=False)[\"stop\"]\n",
    "                   .count().rename(columns={\"stop\":\"pitStops\"}))\n",
    "stints[\"tireStints\"] = stints[\"pitStops\"] + 1\n",
    "\n",
    "# Average pit duration (ms)\n",
    "pit_stops[\"duration_ms\"] = pd.to_numeric(pit_stops[\"milliseconds\"], errors=\"coerce\")\n",
    "pit_agg = (pit_stops.groupby([\"raceId\",\"driverId\"], as_index=False)[\"duration_ms\"]\n",
    "                   .mean().rename(columns={\"duration_ms\":\"avgPitMs\"}))\n",
    "\n",
    "# Optional: Compound shares per race (if you have compound per lap or per stint)\n",
    "# If not available, set zeros; your server can default to 0\n",
    "tire_feats = stints.merge(pit_agg, on=[\"raceId\",\"driverId\"], how=\"outer\")\n",
    "tire_feats[\"avgPitMs\"] = tire_feats[\"avgPitMs\"].fillna(0)\n",
    "tire_feats[\"tireStints\"] = tire_feats[\"tireStints\"].fillna(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "eb652ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Target cleanup & outliers\n",
    "# =====================================================\n",
    "Y = Y.copy()\n",
    "\n",
    "# Keep only classified finishers with real positions (1..20)\n",
    "Y = Y[(Y[\"positionOrder\"].notna()) & (Y[\"positionOrder\"] > 0)]\n",
    "Y[\"finish_pos\"] = Y[\"positionOrder\"].clip(1, 20)\n",
    "\n",
    "# Remove grid==0 entries (no proper start position)\n",
    "Y = Y[Y[\"grid\"] > 0]\n",
    "\n",
    "# (Optional) drop DNS/DNF by status if you joined 'status.csv'\n",
    "# status = pd.read_csv(DATA_DIR / \"status.csv\")\n",
    "# Y = Y.merge(status[['statusId','status']], on='statusId', how='left')\n",
    "# Y = Y[~Y['status'].str.contains(\"DNF|DSQ|DNS\", na=False)]\n",
    "\n",
    "# Light winsorize on pit durations\n",
    "if \"avgPitMs\" in Y.columns:\n",
    "    Y[\"avgPitMs\"] = Y[\"avgPitMs\"].clip(\n",
    "        lower=Y[\"avgPitMs\"].quantile(0.01),\n",
    "        upper=Y[\"avgPitMs\"].quantile(0.99)\n",
    "    )\n",
    "\n",
    "# ðŸš¦ Filter chaotic races: require at least 16 classified finishers in the race\n",
    "finishers = (results.assign(classified=(results['positionOrder'] > 0).astype(int))\n",
    "                    .groupby('raceId', as_index=False)['classified'].sum()\n",
    "                    .rename(columns={'classified': 'n_finishers'}))\n",
    "Y = Y.merge(finishers, on='raceId', how='left')\n",
    "Y = Y[Y['n_finishers'] >= 16]\n",
    "\n",
    "\n",
    "# (If you also filter to modern era, do it AFTER this)\n",
    "# Y = Y[Y['year'] >= 2014]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "c3795d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to modern era (Hybrid + DRS era)\n",
    "Y = Y[Y['year'] >= 2014]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "665ef23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 9. Train/valid split (grouped by year)\n",
    "# =====================================================\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "feature_cols = [\n",
    "    \"grid\",\"pit_count\",\"pit_total_duration\",\"pit_avg_duration\",\n",
    "    \"first_pit_lap\",\"last_pit_lap\",\n",
    "    \"circuit_overtake_difficulty\",\"round\",\"circuitId\",\"country\",\n",
    "    \"carPerformanceIndex\",\"tireStints\",\"avgPitMs\",\n",
    "    \"first_stop_delta\", \"tire_aggr_index\"  # <--- ADD THESE\n",
    "]\n",
    "\n",
    "# Keep only columns that actually exist\n",
    "feature_cols = [c for c in feature_cols if c in Y.columns]\n",
    "X = Y[feature_cols].copy()\n",
    "y = Y[\"finish_pos\"].astype(float)\n",
    "groups = Y[\"raceId\"]\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, valid_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "55955aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 10. Model: ColumnTransformer + xgb.train (MAE + early stopping on old xgboost)\n",
    "# =====================================================\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import DMatrix, train as xgb_train\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "numeric_features = [c for c in feature_cols if c not in [\"country\"]]\n",
    "categorical_features = [c for c in feature_cols if c in [\"country\"]]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",\"passthrough\", numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Fit preprocess, transform splits\n",
    "prep_fitted = preprocess.fit(X_train, y_train)\n",
    "X_train_t = prep_fitted.transform(X_train)\n",
    "X_valid_t = prep_fitted.transform(X_valid)\n",
    "\n",
    "dtrain = DMatrix(X_train_t, label=y_train.values)\n",
    "dvalid = DMatrix(X_valid_t, label=y_valid.values)\n",
    "\n",
    "# monotonicity: higher grid -> worse finish\n",
    "mono = [1 if col == \"grid\" else 0 for col in numeric_features]\n",
    "monotone_str = \"(\" + \",\".join(str(v) for v in mono) + \")\"\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:absoluteerror\",  # MAE\n",
    "    \"eta\": 0.03,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"min_child_weight\": 4,\n",
    "    \"alpha\": 1.0,\n",
    "    \"lambda\": 3.0,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"seed\": 42,\n",
    "    \"monotone_constraints\": monotone_str,\n",
    "    \"eval_metric\": \"mae\",\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "booster = xgb_train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=2000,\n",
    "    evals=watchlist,\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# A tiny wrapper so we can reuse in the rest of the notebook like a sklearn estimator\n",
    "class BoosterWrapper:\n",
    "    def __init__(self, booster, preprocessor):\n",
    "        self.booster = booster\n",
    "        self.preprocessor = preprocessor\n",
    "    def predict(self, X):\n",
    "        Xt = self.preprocessor.transform(X)\n",
    "        return self.booster.predict(DMatrix(Xt))\n",
    "\n",
    "# 'pipe' compatible object with .predict(X)\n",
    "pipe = BoosterWrapper(booster, prep_fitted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "6c2044d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 2.748 | RMSE: 3.880\n",
      "Baseline (finishâ‰ˆgrid) MAE: 3.5421286031042127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# RAW features only â€” the wrapper/pipeline will transform internally\n",
    "pred_valid_raw = pipe.predict(X_valid)\n",
    "pred_valid = np.clip(pred_valid_raw, 1, 20)\n",
    "\n",
    "mae  = mean_absolute_error(y_valid, pred_valid)\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, pred_valid))\n",
    "print(f\"Validation MAE: {mae:.3f} | RMSE: {rmse:.3f}\")\n",
    "\n",
    "baseline_grid = np.clip(X_valid[\"grid\"].values, 1, 20)\n",
    "print(\"Baseline (finishâ‰ˆgrid) MAE:\", mean_absolute_error(y_valid, baseline_grid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "4e2b7ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top features:\n",
      "num__grid                                0.2041\n",
      "num__pit_count                           0.0621\n",
      "num__last_pit_lap                        0.0545\n",
      "num__carPerformanceIndex                 0.0471\n",
      "num__tire_aggr_index                     0.0306\n",
      "num__first_stop_delta                    0.0273\n",
      "num__first_pit_lap                       0.0263\n",
      "num__pit_avg_duration                    0.0231\n",
      "cat__country_Belgium                     0.0215\n",
      "cat__country_Netherlands                 0.0203\n",
      "cat__country_Australia                   0.0201\n",
      "num__pit_total_duration                  0.0195\n",
      "cat__country_Qatar                       0.0194\n",
      "cat__country_Mexico                      0.0192\n",
      "num__circuitId                           0.0192\n",
      "cat__country_Spain                       0.0190\n",
      "cat__country_Singapore                   0.0183\n",
      "cat__country_China                       0.0182\n",
      "cat__country_Germany                     0.0179\n",
      "num__circuit_overtake_difficulty         0.0178\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Feature importances (works for Pipeline, XGBRegressor, or Booster)\n",
    "# =====================================================\n",
    "import numpy as np\n",
    "\n",
    "# 1) Identify model (Pipeline or raw Booster)\n",
    "mdl = None\n",
    "prep = None\n",
    "try:\n",
    "    # If you trained a Pipeline: pipe = Pipeline([(\"prep\", preprocess), (\"model\", xgb)])\n",
    "    mdl = pipe.named_steps[\"model\"]\n",
    "    prep = pipe.named_steps.get(\"prep\", None)\n",
    "except Exception:\n",
    "    # If you stored the raw model in `pipe`\n",
    "    mdl = pipe\n",
    "    try:\n",
    "        prep = preprocess  # your ColumnTransformer, if you kept it in a variable named 'preprocess'\n",
    "    except NameError:\n",
    "        prep = None\n",
    "\n",
    "# 2) Get underlying Booster\n",
    "try:\n",
    "    booster = mdl.get_booster()     # XGBRegressor -> Booster\n",
    "except Exception:\n",
    "    booster = getattr(mdl, \"booster\", None) or mdl  # sometimes it's already a Booster\n",
    "\n",
    "# 3) Importance dict (try several types)\n",
    "imp = booster.get_score(importance_type=\"gain\")\n",
    "if not imp:\n",
    "    imp = booster.get_score(importance_type=\"weight\")\n",
    "if not imp:\n",
    "    imp = booster.get_score(importance_type=\"cover\")\n",
    "if not imp:\n",
    "    raise ValueError(\"Booster returned empty importance dict. Make sure the model is fitted.\")\n",
    "\n",
    "# 4) Feature names\n",
    "feat_names = None\n",
    "if prep is not None:\n",
    "    try:\n",
    "        feat_names = list(prep.get_feature_names_out())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if feat_names is None:\n",
    "    # Infer count from keys (f0..fN-1), else fall back to length of dict\n",
    "    try:\n",
    "        max_idx = max(int(k[1:]) for k in imp.keys() if str(k).startswith(\"f\") and str(k[1:]).isdigit())\n",
    "        n_feats = max_idx + 1\n",
    "    except Exception:\n",
    "        # last resort: if X_train is in scope and has a column count, use that\n",
    "        if \"X_train\" in globals():\n",
    "            n_feats = X_train.shape[1]\n",
    "        else:\n",
    "            n_feats = len(imp)\n",
    "    feat_names = [f\"f{i}\" for i in range(n_feats)]\n",
    "\n",
    "# 5) Map importance to a dense array aligned to feature indices\n",
    "scores = np.zeros(len(feat_names), dtype=float)\n",
    "for k, v in imp.items():\n",
    "    if k.startswith(\"f\") and k[1:].isdigit():\n",
    "        idx = int(k[1:])\n",
    "        if 0 <= idx < len(scores):\n",
    "            scores[idx] = float(v)\n",
    "    else:\n",
    "        # sometimes keys are real feature names (rare with Booster)\n",
    "        try:\n",
    "            idx = feat_names.index(k)\n",
    "            scores[idx] = float(v)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "# Normalize for readability (optional)\n",
    "total = scores.sum()\n",
    "if total > 0:\n",
    "    scores = scores / total\n",
    "\n",
    "pairs = list(zip(feat_names, scores))\n",
    "pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "top = pairs[:20]\n",
    "\n",
    "print(\"\\nTop features:\")\n",
    "for n, w in top:\n",
    "    print(f\"{n:40s} {w:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9ebf2809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (finish=grid) MAE: 3.549889135254989\n",
      "Pseudo baseline MAE: 5.3046575982538355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Baseline 1: \"finish = grid\"\n",
    "baseline_grid_mae = mean_absolute_error(y_valid, X_valid['grid'])\n",
    "print(\"Baseline (finish=grid) MAE:\", baseline_grid_mae)\n",
    "\n",
    "# Baseline 2: only car pace + overtake + circuit (very rough)\n",
    "import numpy as np\n",
    "pseudo = (\n",
    "    21\n",
    "    - 10 * X_valid['carPerformanceIndex'].fillna(0.5)\n",
    "    - 3  * X_valid['circuit_overtake_difficulty'].fillna(0.5)\n",
    ")\n",
    "pseudo = np.clip(pseudo, 1, 20)\n",
    "print(\"Pseudo baseline MAE:\", mean_absolute_error(y_valid, pseudo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "85a6af57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model + serve_schema.json\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 12. Save artifacts & helpers\n",
    "# =====================================================\n",
    "from joblib import dump\n",
    "import json\n",
    "\n",
    "ARTIFACTS_DIR = Path(\"./artifacts\")\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# pack a tiny schema alongside the model (what server must provide)\n",
    "serve_schema = {\n",
    "    \"numeric\": numeric_features,       # <--- Changed from numeric_cols\n",
    "    \"categorical\": categorical_features # <--- Changed from cat_cols\n",
    "}\n",
    "\n",
    "with open(ARTIFACTS_DIR / \"serve_schema.json\", \"w\") as f:\n",
    "    json.dump(serve_schema, f, indent=2)\n",
    "\n",
    "# save model (keep same filename if server expects it)\n",
    "dump(pipe, ARTIFACTS_DIR / \"finish_regressor_xgb_v2.pkl\")\n",
    "dump(pipe, ARTIFACTS_DIR / \"finish_regressor_xgb.pkl\")  # overwrite current for deployment\n",
    "print(\"Saved model + serve_schema.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
